{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C. 고급 응용 분석 - 토픽 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python3.12.6 사용중"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필요한 라이브러리 설치."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pyLDAvis tomotopy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Downloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting requests[socks] (from gdown)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tqdm (from gdown)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4->gdown)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests[socks]->gdown)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests[socks]->gdown)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests[socks]->gdown)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests[socks]->gdown)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\n",
      "Using cached filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, soupsieve, PySocks, idna, filelock, charset_normalizer, certifi, requests, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.5 certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 gdown-5.2.0 idna-3.10 requests-2.32.5 soupsieve-2.8 tqdm-4.67.1 typing-extensions-4.15.0 urllib3-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown  # 구글 드라이브 파일 다운로드에 활용\n",
    "import pandas as pd  # 데이터프레임 처리, CSV/JSON 입출력 등\n",
    "from typing import List, Dict, Any, Tuple, Optional  # 타입 힌트 제공\n",
    "#pyLDAvis과 numpy 라이브러리 충돌문제 해결, 런타임 재실행없이 실행하기 위한 디버깅코드\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pyLDAvis\n",
    "import tomotopy as tp\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fgVYI6UrVs96zO6sAFcirwJhPCLvZQRv\n",
      "To: /Users/irolim/Documents/한중연/4차시/논자시/exam_2025/IRI이미지형용사_definitions_tokens_kiwi.csv\n",
      "100%|██████████| 828k/828k [00:00<00:00, 1.38MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IRI이미지형용사_definitions_tokens_kiwi.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Google Drive 파일 ID\n",
    "file_id = \"1fgVYI6UrVs96zO6sAFcirwJhPCLvZQRv\" #임이로 기존 데이터 사용.\n",
    "output = \"IRI이미지형용사_definitions_tokens_kiwi.csv\"\n",
    "\n",
    "# 2) 파일 다운로드 (gdown 사용)\n",
    "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output, quiet=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 말뭉치 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus_for_topic(token_df: pd.DataFrame, token_col: str = \"norm_token\", min_len: int = 2) -> Tuple[List[List[str]], List[int]]:\n",
    "    \"\"\"\n",
    "    문서별 토큰 리스트를 반환\n",
    "    - min_len: 최소 토큰 길이 (짧은 조사/불용어 제거용)\n",
    "    \"\"\"\n",
    "    # 만약 token_df에 \"doc_id\" 컬럼이 없으면\n",
    "    # definition 단위로 그룹을 묶어 새로운 doc_id 생성\n",
    "    if \"doc_id\" not in token_df.columns:\n",
    "        token_df[\"doc_id\"] = token_df.groupby(\"definition\").ngroup()\n",
    "\n",
    "    docs: List[List[str]] = [] # 문서별 토큰 리스트\n",
    "    doc_ids: List[int] = [] # 문서 ID 리스트\n",
    "    # doc_id별 그룹화 후 토큰 모으기\n",
    "    for doc_id, group in token_df.groupby(\"doc_id\"):\n",
    "        # NaN 제거 → 문자열 변환 → 리스트화\n",
    "        # min_len보다 짧은 토큰은 제거 (예: \"은\", \"가\" 같은 조사)\n",
    "        tokens = [t for t in group[token_col].dropna().astype(str).tolist() if len(t) >= min_len]\n",
    "        # 유효 토큰이 존재하는 문서만 결과에 추가\n",
    "        if tokens:\n",
    "            docs.append(tokens) # 문서 단위 토큰 리스트\n",
    "            doc_ids.append(doc_id) # 해당 문서 ID 저장\n",
    "    # (문서별 토큰 리스트, 문서 ID 리스트) 반환\n",
    "    return docs, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda(docs: List[List[str]], num_topics: int = 10, seed: int = 42, min_cf: int = 3, rm_top: int = 0, iterations: int = 200) -> tp.LDAModel:\n",
    "    \"\"\"\n",
    "    tomotopy LDA 모델 학습\n",
    "    - num_topics: 토픽 개수\n",
    "    - min_cf: 최소 출현 빈도 (저빈도 단어 제거)\n",
    "    - rm_top: 상위 몇 개의 고빈도 단어 제거\n",
    "    - iterations: 학습 반복 횟수\n",
    "    \"\"\"\n",
    "    # LDA 모델 초기화\n",
    "    mdl = tp.LDAModel(\n",
    "        k=num_topics,  # 토픽 개수\n",
    "        seed=seed,     # 랜덤 시드\n",
    "        min_cf=min_cf, # 최소 출현 빈도\n",
    "        rm_top=rm_top  # 상위 고빈도 단어 제거\n",
    "    )\n",
    "    # 문서별 토큰 리스트를 모델에 추가\n",
    "    for doc in docs:\n",
    "        mdl.add_doc(doc)\n",
    "    # 모델 기본 정보 출력\n",
    "    print(f\"[LDA] 총 문서 수: {len(mdl.docs)}, 어휘 수: {mdl.num_vocabs}, 토픽 수: {mdl.k}\")\n",
    "\n",
    "    # 지정된 반복(iterations) 동안 학습 진행\n",
    "    # 20회 단위로 끊어서 perplexity(혼잡도) 출력\n",
    "    for i in range(0, iterations, 20):\n",
    "        mdl.train(20)\n",
    "        print(f\"Iteration: {i+20}\\tPerplexity: {mdl.perplexity:.4f}\")\n",
    "\n",
    "\n",
    "    return mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적 토픽 수 결정 (Perplexity / Coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_k(docs: List[List[str]], k_range: range, iterations: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    LDA에서 최적 토픽 개수(k)를 찾기 위한 함수\n",
    "\n",
    "    매개변수:\n",
    "    - docs: 문서별 토큰 리스트 (list of list 형태)\n",
    "    - k_range: 실험할 토픽 개수 범위 (예: range(5, 16, 1))\n",
    "    - iterations: 각 k값마다 학습 반복 횟수 (기본 100)\n",
    "\n",
    "    반환값:\n",
    "    - 결과 DataFrame (컬럼: k, perplexity, coherence)\n",
    "      - perplexity: 혼잡도 (낮을수록 모델이 데이터를 잘 설명)\n",
    "      - coherence: 토픽 일관성 점수 (높을수록 해석 가능성이 높음)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # 주어진 범위의 k 값에 대해 반복\n",
    "    for k in k_range:\n",
    "        # LDA 모델 초기화 (k개 토픽, 최소 출현 빈도=3)\n",
    "        mdl = tp.LDAModel(k=k, seed=42, min_cf=3)\n",
    "\n",
    "        # 문서 추가\n",
    "        for doc in docs:\n",
    "            mdl.add_doc(doc)\n",
    "\n",
    "        # 모델 학습\n",
    "        mdl.train(iterations)\n",
    "\n",
    "        # Perplexity 계산 (모델의 적합도, 낮을수록 좋음)\n",
    "        perp = mdl.perplexity\n",
    "\n",
    "        # Coherence 계산 (토픽 해석 가능성, 높을수록 좋음)\n",
    "        coh = tp.coherence.Coherence(mdl, coherence='c_v').get_score()\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append({\"k\": k, \"perplexity\": perp, \"coherence\": coh})\n",
    "\n",
    "        # 중간 결과 출력\n",
    "        print(f\"k={k} -> Perplexity={perp:.4f}, Coherence={coh:.4f}\")\n",
    "\n",
    "    # k별 결과를 DataFrame으로 반환\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토픽 주요 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 토픽(k=mdl.k)에 대해 순회하며 get_topic_words 호출\n",
    "def get_topic_words(mdl: tp.LDAModel, top_n: int = 10) -> Dict[int, List[Tuple[str, float]]]:\n",
    "    return {k: mdl.get_topic_words(k, top_n=top_n) for k in range(mdl.k)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서별 토픽 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서-토픽 분포(Document-Topic Distribution) 추출 함수\n",
    "def get_doc_topic_dist(mdl: tp.LDAModel) -> pd.DataFrame:\n",
    "  # 각 문서(doc)에 대해 토픽 분포 벡터 추출\n",
    "    doc_topics = [doc.get_topic_dist() for doc in mdl.docs]\n",
    "    # DataFrame으로 변환, 컬럼명은 topic_0, topic_1, ...\n",
    "    return pd.DataFrame(doc_topics, columns=[f\"topic_{i}\" for i in range(mdl.k)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pyLDAvis 시각화 (tomotopy.visualize 없을 때도 동작)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_ldavis_fallback(mdl: tp.LDAModel):\n",
    "    \"\"\"\n",
    "    tomotopy.visualize 모듈이 없는 경우(구버전 등)를 대비한 pyLDAvis 준비 함수\n",
    "    - wid가 int 또는 str인 경우 모두 처리\n",
    "    - term_frequency, doc_lengths, topic_term_dists, doc_topic_dists 구성\n",
    "    \"\"\"\n",
    "    # --- 어휘 목록 및 매핑 ---\n",
    "    vocab = list(mdl.used_vocabs)        # 모델이 실제 사용한 단어 목록\n",
    "    V = len(vocab)                       # 전체 어휘 수\n",
    "    vocab2id = {w: i for i, w in enumerate(vocab)}  # 단어→인덱스 매핑\n",
    "\n",
    "    # --- term_frequency (길이 V): 전체 코퍼스에서 각 단어의 출현 빈도 ---\n",
    "    term_frequency = np.zeros(V, dtype=np.int64)\n",
    "    for d in mdl.docs:\n",
    "        try:\n",
    "            pairs = d.get_words()  # (wid, cnt) 형태 리스트\n",
    "        except AttributeError:\n",
    "            pairs = None\n",
    "\n",
    "        if pairs is not None:\n",
    "            for wid, cnt in pairs:\n",
    "                # wid가 int/np.integer일 때\n",
    "                if isinstance(wid, (int, np.integer)):\n",
    "                    if 0 <= wid < V:\n",
    "                        term_frequency[wid] += int(cnt)\n",
    "                # wid가 str(토큰)일 때\n",
    "                elif isinstance(wid, str):\n",
    "                    idx = vocab2id.get(wid)\n",
    "                    if idx is not None:\n",
    "                        term_frequency[idx] += int(cnt)\n",
    "        else:\n",
    "            # 구형 tomotopy에서는 d.words 속성이 wid 리스트일 수 있음\n",
    "            for wid in getattr(d, \"words\", []):\n",
    "                if isinstance(wid, (int, np.integer)):\n",
    "                    if 0 <= wid < V:\n",
    "                        term_frequency[wid] += 1\n",
    "                elif isinstance(wid, str):\n",
    "                    idx = vocab2id.get(wid)\n",
    "                    if idx is not None:\n",
    "                        term_frequency[idx] += 1\n",
    "\n",
    "    # --- topic_term_dists (K x V): 각 토픽의 단어 분포 φ ---\n",
    "    topic_term = np.vstack([\n",
    "        np.asarray(mdl.get_topic_word_dist(k), dtype=np.float64)\n",
    "        for k in range(mdl.k)\n",
    "    ])\n",
    "    topic_term = (topic_term + 1e-12) / (topic_term + 1e-12).sum(axis=1, keepdims=True)\n",
    "\n",
    "    # --- doc_topic_dists (D x K): 각 문서의 토픽 분포 θ ---\n",
    "    doc_topic = np.vstack([\n",
    "        np.asarray(doc.get_topic_dist(), dtype=np.float64)\n",
    "        for doc in mdl.docs\n",
    "    ])\n",
    "    doc_topic = (doc_topic + 1e-12) / (doc_topic + 1e-12).sum(axis=1, keepdims=True)\n",
    "\n",
    "    # --- 문서 길이 계산 (단어 수) ---\n",
    "    doc_lengths = []\n",
    "    for d in mdl.docs:\n",
    "        try:\n",
    "            pairs = d.get_words()\n",
    "            doc_lengths.append(int(sum(int(cnt) for _, cnt in pairs)))\n",
    "        except AttributeError:\n",
    "            wl = getattr(d, \"words\", [])\n",
    "            doc_lengths.append(int(len(wl)))\n",
    "\n",
    "    # pyLDAvis용 데이터 준비\n",
    "    return pyLDAvis.prepare(\n",
    "        topic_term_dists=topic_term,\n",
    "        doc_topic_dists=doc_topic,\n",
    "        doc_lengths=doc_lengths,\n",
    "        vocab=vocab,\n",
    "        term_frequency=term_frequency,\n",
    "        sort_topics=False\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize_lda(mdl: tp.LDAModel, outfile: str = \"토픽모델_시각화.html\"):\n",
    "    \"\"\"\n",
    "    LDA 토픽 모델 시각화 함수\n",
    "    1순위: tomotopy.visualize 모듈이 있으면 사용\n",
    "    2순위: _prepare_ldavis_fallback()으로 pyLDAvis 시각화 준비\n",
    "    - 결과는 HTML 파일로 저장\n",
    "    - 기본 저장 파일명: '토픽모델_시각화.html' (한글 파일명)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tomotopy.visualize as tpvis\n",
    "        prepared = tpvis.prepare(mdl)\n",
    "    except ImportError:\n",
    "        prepared = _prepare_ldavis_fallback(mdl)\n",
    "\n",
    "    # pyLDAvis 시각화 결과를 HTML 파일로 저장\n",
    "    pyLDAvis.save_html(prepared, outfile)\n",
    "    print(f\"✅ 토픽 모델 시각화 저장 완료: {outfile}\")\n",
    "    return prepared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수 실행 (문서별 토픽 분포, 시각화.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/mj8pxhhd6ml7kpdhggf19l_80000gn/T/ipykernel_46708/3608272736.py:27: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  mdl.train(iterations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5 -> Perplexity=287.2808, Coherence=0.6656\n",
      "k=6 -> Perplexity=290.8404, Coherence=0.7309\n",
      "k=7 -> Perplexity=279.0569, Coherence=0.7686\n",
      "k=8 -> Perplexity=292.7083, Coherence=0.7855\n",
      "k=9 -> Perplexity=284.5378, Coherence=0.8101\n",
      "k=10 -> Perplexity=288.5440, Coherence=0.8074\n",
      "k=11 -> Perplexity=290.1807, Coherence=0.8186\n",
      "k=12 -> Perplexity=292.3652, Coherence=0.8316\n",
      "k=13 -> Perplexity=287.2897, Coherence=0.8376\n",
      "k=14 -> Perplexity=306.6374, Coherence=0.8547\n",
      "k=15 -> Perplexity=278.8317, Coherence=0.8611\n",
      "\n",
      "[최적 토픽 수 후보]\n",
      "     k  perplexity  coherence\n",
      "0    5  287.280758   0.665609\n",
      "1    6  290.840389   0.730853\n",
      "2    7  279.056931   0.768597\n",
      "3    8  292.708309   0.785465\n",
      "4    9  284.537838   0.810142\n",
      "5   10  288.543976   0.807415\n",
      "6   11  290.180735   0.818647\n",
      "7   12  292.365160   0.831590\n",
      "8   13  287.289675   0.837615\n",
      "9   14  306.637365   0.854724\n",
      "10  15  278.831727   0.861144\n",
      "[LDA] 총 문서 수: 491, 어휘 수: 0, 토픽 수: 8\n",
      "Iteration: 20\tPerplexity: 353.1553\n",
      "Iteration: 40\tPerplexity: 322.2671\n",
      "Iteration: 60\tPerplexity: 303.4906\n",
      "Iteration: 80\tPerplexity: 299.0780\n",
      "Iteration: 100\tPerplexity: 292.7083\n",
      "Iteration: 120\tPerplexity: 275.6468\n",
      "Iteration: 140\tPerplexity: 272.1553\n",
      "Iteration: 160\tPerplexity: 269.1628\n",
      "Iteration: 180\tPerplexity: 268.1086\n",
      "Iteration: 200\tPerplexity: 265.6215\n",
      "\n",
      "[토픽 0]\n",
      "거나:0.152, 않다:0.094, 에서:0.079, 강의:0.065, 관계:0.065, 또는:0.051, 까지:0.051, 세다:0.036, 예민:0.029, 관하다:0.029\n",
      "\n",
      "[토픽 1]\n",
      "이나:0.124, 못하다:0.08, 있다:0.073, 느낌:0.055, 성격:0.048, 태도:0.044, 분위기:0.029, 따뜻:0.029, 약하다:0.026, 깨끗:0.022\n",
      "\n",
      "[토픽 2]\n",
      "마음:0.101, 있다:0.083, 이다:0.072, 어서:0.057, 소리:0.054, 상태:0.043, 또는:0.043, 강하다:0.04, 약하다:0.036, 감정:0.032\n",
      "\n",
      "[토픽 3]\n",
      "꾸미다:0.088, 사회:0.088, 보다:0.066, 발전:0.066, 아름답다:0.066, 새롭다:0.055, 지키다:0.044, 변화:0.044, 방법:0.044, 충분히:0.033\n",
      "\n",
      "[토픽 4]\n",
      "않다:0.164, 이나:0.151, 없다:0.098, 거나:0.054, 생각:0.041, 있다:0.041, 자연:0.032, 분명:0.032, 사람:0.032, 그렇다:0.016\n",
      "\n",
      "[토픽 5]\n",
      "움직이다:0.098, 으로:0.093, 않다:0.084, 일정:0.049, 되다:0.049, 움직임:0.04, 거나:0.04, 없이:0.036, 스스로:0.027, 받다:0.027\n",
      "\n",
      "[토픽 6]\n",
      "어떤:0.124, 하다:0.108, 이나:0.071, 지다:0.03, 약속:0.03, 으로:0.025, 에게:0.023, 정하다:0.023, 확실:0.023, 사람:0.023\n",
      "\n",
      "[토픽 7]\n",
      "거나:0.123, 좋다:0.099, 행동:0.063, 정도:0.055, 에서:0.052, 이나:0.052, 매우:0.04, 높다:0.04, 기분:0.032, 이성:0.032\n",
      "\n",
      "문서별 토픽 분포 저장 완료: 문서별_토픽분포.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/mj8pxhhd6ml7kpdhggf19l_80000gn/T/ipykernel_46708/2143239419.py:25: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  mdl.train(20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토픽 모델 시각화 저장 완료: 토픽모델_시각화.html\n",
      "\n",
      "토픽 모델 시각화 저장 완료: 토픽모델_시각화.html\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Part B에서 저장한 token_df 불러오기\n",
    "    token_df = pd.read_csv(output, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # 'norm_token' 컬럼이 없다면 lemma/form 기반으로 생성\n",
    "    if \"norm_token\" not in token_df.columns:\n",
    "        token_df[\"norm_token\"] = token_df[\"tok_lemma\"].fillna(token_df[\"tok_form\"])\n",
    "\n",
    "    # 1) 말뭉치 생성 (문서별 토큰 리스트와 ID)\n",
    "    docs, doc_ids = build_corpus_for_topic(token_df, token_col=\"norm_token\", min_len=2)\n",
    "\n",
    "    # 2) 최적 토픽 수 탐색 (예: 5~15 범위)\n",
    "    results_df = find_optimal_k(docs, k_range=range(5, 16), iterations=100)\n",
    "    print(\"\\n[최적 토픽 수 후보]\")\n",
    "    print(results_df)\n",
    "\n",
    "    # 3) LDA 모델 학습 (예: k=8 선택)\n",
    "    mdl = train_lda(docs, num_topics=8, iterations=200)\n",
    "\n",
    "    # 4) 각 토픽 주요 단어 출력\n",
    "    topic_words = get_topic_words(mdl, top_n=10)\n",
    "    for k, words in topic_words.items():\n",
    "        print(f\"\\n[토픽 {k}]\")\n",
    "        print(\", \".join([f\"{w}:{round(p,3)}\" for w, p in words]))\n",
    "\n",
    "    # 5) 문서별 토픽 분포 저장 (CSV, 한글 파일명)\n",
    "    doc_topic_df = get_doc_topic_dist(mdl)\n",
    "    doc_topic_df.to_csv(\"문서별_토픽분포.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"\\n문서별 토픽 분포 저장 완료: 문서별_토픽분포.csv\")\n",
    "\n",
    "    # 6) 시각화 결과 저장 (HTML, 한글 파일명)\n",
    "    vis = visualize_lda(mdl, outfile=\"토픽모델_시각화.html\")\n",
    "    print(\"\\n토픽 모델 시각화 저장 완료: 토픽모델_시각화.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
